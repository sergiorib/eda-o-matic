{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy \n",
    "from typing import List, Dict, Any, Optional\n",
    "from charset_normalizer import from_path \n",
    "\n",
    "MAIN_PATH = Path().resolve().parent\n",
    "sys.path.append(str(MAIN_PATH))\n",
    "\n",
    "from src.utilities import load_config, load_validations, load_fields, load_data, init_log, format_file_size\n",
    "\n",
    "import src.analisys \n",
    "from src.analisys import field_apply_list, check_null_empty, check_values_list, check_regex_format, check_zero_values, check_negative_values, check_valid_range\n",
    "\n",
    "from src.utilities.logger import log_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega as configura√ß√µes e inicia o log \n",
    "df_config = pd.DataFrame([{}])\n",
    "load_config(df_config)\n",
    "\n",
    "# inicia o log com o path parametrizado\n",
    "init_log(df_config.loc[0, \"log_path\"])\n",
    "\n",
    "# Carrega a lista de valida√ß√µes\n",
    "df_validations = pd.DataFrame([{}])\n",
    "load_validations(df_validations,df_config.loc[0, \"eda_config_path\"] )\n",
    "\n",
    "# Carrega a lista de campos a validar\n",
    "df_fields = pd.DataFrame([{}])\n",
    "load_fields(df_fields, df_config.loc[0, \"eda_config_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "result: List[Dict[str, Any]] = []\n",
    "# rotina pra salvar os resutlados das analises \n",
    "def save_result(\n",
    "    file: str, field: str, category: str, test: str, evidence: Any, detail: Optional[str] = None, status: str = \"PASS\") -> result:    \n",
    "    registry: result = { \"file\": file, \"Field\": field, \"category\": category, \"test\": test, \"evidence\": evidence, \"detail\": detail, \"status\": status,}\n",
    "    return registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdb464",
   "metadata": {},
   "source": [
    "Inicializa variaveis para leitura dos campos configurados para analise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c24e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame([{}])\n",
    "FAIL_TABLES = set()   # Controla falhas estruturais (Passo 1)\n",
    "loaded_file = \"\"\n",
    "separator = df_config.loc[0, \"separator\"]\n",
    "encode = df_config.loc[0, \"encode\"]\n",
    "new_file = True \n",
    "RESULTS = [] \n",
    "\n",
    "# Converte todos os campos para str\n",
    "df_fields = df_fields.astype('string')\n",
    "\n",
    "# Deixa todos os nomes de campos em minusculo e remove espa√ßos\n",
    "df_fields.columns = df_fields.columns.str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7949e21",
   "metadata": {},
   "source": [
    "Inicio do Loop no campo a serem analisados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_fields.iterrows():\n",
    "\n",
    "    file_name = row[\"file\"]\n",
    "    table_name = row[\"table\"]\n",
    "    file_path = df_config.loc[0, \"data_path\"] + file_name\n",
    "\n",
    "    # Verifica se o arquivo est√° carregado ou j√° falhou\n",
    "    if (file_name in FAIL_TABLES): \n",
    "        continue \n",
    "    \n",
    "    if (row[\"file\"] != loaded_file):       \n",
    "        try: \n",
    "            try: # Carregamento do arquivo de dados\n",
    "\n",
    "                # Detecta o encode, se n√£o foi fornecido\n",
    "                try:\n",
    "                    if len(encode) == 0: \n",
    "                        detectado = from_path(str(file_path)).best() \n",
    "                        encoding_detectado = detectado.encoding \n",
    "                    else: \n",
    "                        encoding_detectado = encode\n",
    "                except Exception:\n",
    "                    encoding_detectado = 'utf-8' \n",
    "\n",
    "                # data Load    \n",
    "                df_data = pd.read_csv(file_path,encoding=encoding_detectado,sep=separator,engine='python')\n",
    "\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Falha no carregamento do arquivo !\")\n",
    "\n",
    "            loaded_file = file_name\n",
    "            new_file = True  \n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"‚ùå Erro de Codifica√ß√£o: O arquivo pode n√£o ser '{encoding}'. Tente outro encoding.\")\n",
    "        \n",
    "        except pd.errors.ParserError as e:\n",
    "            # Este erro geralmente indica problemas de estrutura (delimitadores incorretos, linhas mal formadas)\n",
    "            print(f\"‚ùå Erro de Parsing (Estrutura CSV incorreta): {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Captura outros erros, como FileNotFoundError\n",
    "            print(f\"‚ùå Erro inesperado ao ler o arquivo: {e}\") \n",
    "\n",
    "    if new_file: \n",
    "        print(\"-\" * 30 )\n",
    "        print(table_name)\n",
    "        print(\"-\" * 30 )\n",
    "        # Corrige o tipo das colunas \"int\" exibidas como \"float\" \n",
    "        for col in df_data.select_dtypes(include=[\"float\"]).columns:\n",
    "            if (df_data[col].dropna() % 1 == 0).all():\n",
    "                df_data[col] = df_data[col].astype(\"Int64\")        \n",
    "\n",
    "        # Coleta estatisticas no nivel do arquivo \n",
    "        file_size = format_file_size(os.path.getsize(file_path) )\n",
    "        line_count = len(df_data)\n",
    "        col_count = len(df_data.columns) \n",
    "        evidence_str = \"Tamanho: \" + file_size + \" Linhas/Colunas: \" + str(line_count) + \"/\" + str(col_count)\n",
    "        RESULTS.append(save_result(row[\"file\"],  \"Todos\",\"structure\",\"file info\",evidence_str,\"\", \"pass\")) \n",
    "\n",
    "        # Sanitiza√ß√£o da taberla de dados \n",
    "        # Deixa todos os nomes de campos em minusculo e remove espa√ßos\n",
    "        df_data.columns = df_data.columns.str.strip().str.lower()   \n",
    "        # print(df_data.dtypes)\n",
    "\n",
    "\n",
    "\n",
    "        # Testa colunas faltantes\n",
    "        missing_columns_lst = [] \n",
    "        df_expected_columns = (df_fields[df_fields['table'] == table_name]['field'].unique()) \n",
    "        data_column_names = df_data.columns.to_list()\n",
    "        df_data_columns = pd.DataFrame({\"column\": data_column_names})\n",
    "        expected_set = set([c.strip().lower() for c in df_expected_columns])\n",
    "        data_set = set(df_data_columns['column'].str.strip().str.lower().tolist())\n",
    "        missing_colunms = expected_set.difference(data_set)\n",
    "        missing_columns_lst = list(missing_colunms)\n",
    "        if missing_colunms:\n",
    "            str_missing_columns = \", \".join(sorted(list(missing_colunms)))\n",
    "        else:\n",
    "            str_missing_columns = \"nenhuma\"\n",
    "        \n",
    "        # Testa colunas com nome duplicados\n",
    "        nomes_colunas = [col.strip().lower() for col in df_data.columns.tolist()]\n",
    "        nomes_series = pd.Series(nomes_colunas)\n",
    "        nomes_sem_sufixo = nomes_series.str.replace(r'\\.\\d+$', '', regex=True)\n",
    "        contagem_nomes = nomes_sem_sufixo.value_counts()\n",
    "        nomes_duplicados_series = contagem_nomes[contagem_nomes > 1]\n",
    "        lista_nomes_duplicados = nomes_duplicados_series.index.tolist()\n",
    "        colunas_duplicadas = df_data_columns['column'].value_counts()\n",
    "        if lista_nomes_duplicados:\n",
    "            # Junta os nomes duplicados com \", \"\n",
    "            resultado_string = \", \".join(lista_nomes_duplicados)\n",
    "        else:\n",
    "            resultado_string = \"nenhum\"\n",
    "        if resultado_string == \"nenhum\" and str_missing_columns == \"nenhuma\": \n",
    "            status = \"pass\" \n",
    "        else: \n",
    "            status = \"fail\"\n",
    "\n",
    "        evidence_str =  \"Faltantes: \" +  str_missing_columns + \" Nomes_duplicados: \" + resultado_string          \n",
    "        RESULTS.append(save_result(row[\"file\"], \"Todos\" ,\"structure\",\"Column info\",evidence_str,\"\", status)) \n",
    "            \n",
    "        new_file = False\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"***********  \" + row[\"field\"] + \"  **************\")\n",
    "\n",
    "    # Monta a lista de caracteristicas do campo para chamada dos checagens corresposndentes \n",
    "    apply_list = field_apply_list(df_data, df_fields, row)\n",
    "    print(\"---- Caracteristicas ----\")\n",
    "    for item in  apply_list: \n",
    "        print(item) \n",
    "\n",
    "    # Busca as checagens Ativas correspondentes as caracteristicas do campo  \n",
    "    apply_set = set([item.strip().lower() for item in apply_list]) \n",
    "    mask_apply = df_validations['apply'].apply(lambda x: len(set(str(x).lower().replace(' ', '').split(',')) & apply_set) > 0)\n",
    "    mask_active = (df_validations['active'] == 'yes')\n",
    "    df_filtrado = df_validations[mask_apply&mask_active]\n",
    "\n",
    "    # Loop de chamada das analises \n",
    "    print(\"----- Analises aplicadas ----\")\n",
    "    for index, check_row in df_filtrado.iterrows():\n",
    "        try: \n",
    "            print(\"Teste: \" + check_row[\"test\"]+ \" - Apply: \" + check_row[\"apply\"] ) \n",
    "            if str(row[\"field\"]) in list(missing_columns_lst): # ignora colunas que n√£o foram encontrados\n",
    "                continue  \n",
    "            #  chama a rotina parametrizada para a analise\n",
    "            evidence, status, detail  = getattr(src.analisys,check_row[\"routine\"])(df_data, df_fields, row)  \n",
    "            # Salva o resultado das analise no relat√µrio final\n",
    "            RESULTS.append(save_result(row[\"file\"], row[\"field\"] ,check_row[\"category\"],check_row[\"test\"],evidence,detail, status)) \n",
    "        except Exception as e:\n",
    "            evidence = \"Falha na chamada da rotina de analise\"\n",
    "            status = \"error\"\n",
    "            detail = (f\"Rotina '{check_row[\"routine\"]}': \" f\"{type(e).__name__}: {str(e)}\")\n",
    "            RESULTS.append(save_result(row[\"file\"], row[\"field\"] ,check_row[\"category\"],check_row[\"test\"],evidence,detail, status)) \n",
    "\n",
    "            continue\n",
    "\n",
    "    # Chama as rotinas basicas (qualquer tipo de campo)\n",
    "    # df_basic_checks = df_validations[(df_validations['category'] == 'basic') & (df_validations['active'] == 'yes')]\n",
    "    # print(\"df_basic_checks\")\n",
    "    # print(df_basic_checks)\n",
    "    # for index, check_row in df_basic_checks.iterrows():\n",
    "    #     evidence, status, detail  = getattr(src.analisys,check_row[\"routine\"])(df_data, df_fields, row)\n",
    "    #     print(\"Retorno\")\n",
    "    #     print(evidence, status, detail)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb1819",
   "metadata": {},
   "source": [
    "Fim do loop de analise e de campos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4867c24",
   "metadata": {},
   "source": [
    "Gera o relat√µrio de saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f05649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Exibe os registros de auditoria em um formato tabular otimizado\n",
    "    para visualiza√ß√£o em tela, sem quebras de linha em um mesmo registro.\n",
    "    \"\"\"\n",
    "# 1. Manter a configura√ß√£o do cabe√ßalho √† esquerda (que voc√™ j√° fez)\n",
    "pd.set_option('display.colheader_justify', 'left') \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 2. Definir a ordem das colunas\n",
    "colunas_exibicao = ['status', 'file', 'Field', 'test', 'evidence', 'detail']\n",
    "\n",
    "if RESULTS:\n",
    "    DF_RESULTS = pd.DataFrame(RESULTS)\n",
    "\n",
    "    # --- A CORRE√á√ÉO CR√çTICA ---\n",
    "    # Usar justify='left' no m√©todo to_string() for√ßa TODO o conte√∫do\n",
    "    # (incluindo n√∫meros) a ser alinhado √† esquerda.\n",
    "    tabela_formatada = DF_RESULTS[colunas_exibicao].to_string(justify='left')\n",
    "    \n",
    "    # 3. Imprimir\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"--- üìã REGISTROS DE AUDITORIA ---\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    print(tabela_formatada)\n",
    "    \n",
    "    # 4. Reverter as op√ß√µes (boa pr√°tica)\n",
    "    pd.reset_option('display.colheader_justify')\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "\n",
    "else:\n",
    "    print(\"A lista 'RESULTS' est√° vazia.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-o-matic-HKdgnQC0-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
